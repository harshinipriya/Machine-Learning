{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Homework \n",
    "***\n",
    "**Name**: $<$Harshini Muthukrishnan$>$ \n",
    "\n",
    "**Kaggle Username**: $<$Harshini$>$\n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5pm on Friday February 23rd**. Additionally, you must make at least one submission to the **Kaggle** competition before it closes at **4:59pm on Friday February 23rd**. Submit only this Jupyter notebook to Moodle. Do not compress it using tar, rar, zip, etc. Your solutions to analysis questions should be done in Markdown directly below the associated question.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI5622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "***\n",
    "\n",
    "When people are discussing popular media, there’s a concept of spoilers. That is, critical information about the plot of a TV show, book, or movie that “ruins” the experience for people who haven’t read / seen it yet.\n",
    "\n",
    "The goal of this assignment is to do text classification on forum posts from the website [tvtropes.org](http://tvtropes.org/), to predict whether a post is a spoiler or not. We'll be using the logistic regression classifier provided by sklearn.\n",
    "\n",
    "Unlike previous assignments, the code provided with this assignment has all of the functionality required. Your job is to make the functionality better by improving the features the code uses for text classification.\n",
    "\n",
    "**NOTE**: Because the goal of this assignment is feature engineering, not classification algorithms, you may not change the underlying algorithm or it's parameters\n",
    "\n",
    "This assignment is structured in a way that approximates how classification works in the real world: Features are typically underspecified (or not specified at all). You, the data digger, have to articulate the features you need. You then compete against others to provide useful predictions.\n",
    "\n",
    "It may seem straightforward, but do not start this at the last minute. There are often many things that go wrong in testing out features, and you'll want to make sure your features work well once you've found them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle In-Class Competition \n",
    "***\n",
    "\n",
    "In addition to turning in this notebook on Moodle, you'll also need to submit your predictions on Kaggle, an online tournament site for machine learning competitions. The competition page can be found here:  \n",
    "\n",
    "[https://www.kaggle.com/c/feature-engineering-csci-5622-spring-2018](https://www.kaggle.com/c/feature-engineering-csci-5622-spring-2018)\n",
    "\n",
    "Additionally, a private invite link for the competition has been posted to Piazza. \n",
    "\n",
    "The starter code below has a `model_predict` method which produces a two column CSV file that is correctly formatted for Kaggle (predictions.csv). It should have the example Id as the first column and the prediction (`True` or `False`) as the second column. If you change this format your submissions will be scored as zero accuracy on Kaggle. \n",
    "\n",
    "**Note**: You may only submit **THREE** predictions to Kaggle per day.  Instead of using the public leaderboard as your sole evaluation processes, it is highly recommended that you perform local evaluation using a validation set or cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as sia\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [25 points] Problem 1: Feature Engineering \n",
    "***\n",
    "\n",
    "The `FeatEngr` class is where the magic happens.  In it's current form it will read in the training data and vectorize it using simple Bag-of-Words.  It then trains a model and makes predictions.  \n",
    "\n",
    "25 points of your grade will be generated from your performance on the the classification competition on Kaggle. The performance will be evaluated on accuracy on the held-out test set. Half of the test set is used to evaluate accuracy on the public leaderboard.  The other half of the test set is used to evaluate accuracy on the private leaderboard (which you will not be able to see until the close of the competition). \n",
    "\n",
    "You should be able to significantly improve on the baseline system (i.e. the predictions made by the starter code we've provided) as reported by the Kaggle system.  Additionally, the top **THREE** students from the **PRIVATE** leaderboard at the end of the contest will receive 5 extra credit points towards their Problem 1 score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatEngr:\n",
    "    def __init__(self):\n",
    "        \n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        from sklearn.feature_extraction import text\n",
    "        \n",
    "        self.coeffs = []\n",
    "        self.vectorizer_ngram = TfidfVectorizer( analyzer='word', ngram_range=(2, 2),stop_words='english')\n",
    "        self.vectorizer = TfidfVectorizer(strip_accents = \"ascii\",analyzer='char_wb', ngram_range=(5, 5),sublinear_tf=True,stop_words='english')\n",
    "        self.tropeTransformer = CountVectorizer()\n",
    "        #self.tropeTransformer = TfidfVectorizer(analyzer='word',token_pattern='[A-Z][a-z]{1,}')\n",
    "    \n",
    "    \n",
    "    def build_train_features(self, examples):\n",
    "        \"\"\"\n",
    "        Method to take in training text features and do further feature engineering \n",
    "        Most of the work in this homework will go here, or in similar functions  \n",
    "        :param examples: currently just a list of forum posts  \n",
    "        \"\"\"\n",
    "        from sklearn.pipeline import FeatureUnion\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        \n",
    "        \n",
    "        self.pipeline = Pipeline([('subjectbody', SubjectBodyExtractor()),\n",
    "                            ('union', FeatureUnion(\n",
    "                            transformer_list=\n",
    "                                        [\n",
    "                                            \n",
    "                                                ('sentence_bow', Pipeline([\n",
    "                                                ('selector', ItemSelector(key='sentence')),\n",
    "                                                ('tfidf_bow', self.vectorizer),])),\n",
    "                                            \n",
    "                                                ('sentence_ngram', Pipeline([\n",
    "                                                ('selector', ItemSelector(key='sentence')),\n",
    "                                                ('tfidf_ngram', self.vectorizer_ngram),])),\n",
    "                                                \n",
    "                                                ('trope_transformer', Pipeline([\n",
    "                                                ('selector', ItemSelector(key='trope')),\n",
    "                                                ('trope', self.tropeTransformer),])),\n",
    "                                                \n",
    "                                                #('SentimentTransformer', Pipeline([\n",
    "                                                #('selector', ItemSelector(key='sentence')),\n",
    "                                                #('sentiment', SentimentTransformer()),])),\n",
    "                                                \n",
    "                                                #('TropeSentimentTransformer', Pipeline([\n",
    "                                                #('selector', ItemSelector(key='sentence')),\n",
    "                                                #('tropesentiment', TropeSentimentTransformer()),])),\n",
    "                                            \n",
    "                                                ('CapitalTransformer', Pipeline([\n",
    "                                                ('selector', ItemSelector(key='sentence')),\n",
    "                                                ('capital', CapitalTransformer()),])),\n",
    "                                                \n",
    "                                                ('SpecialTransformer', Pipeline([\n",
    "                                                ('selector', ItemSelector(key='sentence')),\n",
    "                                                ('special', SpecialTransformer()),])),\n",
    "                                                \n",
    "                                                #('WordCountTransformer', Pipeline([\n",
    "                                                #('selector', ItemSelector(key='sentence')),\n",
    "                                                #('wordcount', WordCountTransformer()),])),\n",
    "                                            \n",
    "                                                ('NERTransformer', Pipeline([\n",
    "                                                ('selector', ItemSelector(key='sentence')),\n",
    "                                                ('ner', NERTransformer()),])),\n",
    "                                                \n",
    "                                                #('NERTrope', Pipeline([\n",
    "                                                #('selector', ItemSelector(key='trope')),\n",
    "                                                #('ner_trope', TropeNERTransformer()),])),\n",
    "                                            \n",
    "                                                ('POS', Pipeline([\n",
    "                                                ('selector', ItemSelector(key='sentence')),\n",
    "                                                ('POS', POSTransformer()),])),\n",
    "                                        ],)),\n",
    "                            ])\n",
    "\n",
    "        return self.pipeline.fit_transform(examples)\n",
    "\n",
    "\n",
    "    def get_test_features(self, examples):\n",
    "        \"\"\"\n",
    "        Method to ftake in test text features and transform the same way as train features \n",
    "        :param examples: currently just a list of forum posts  \n",
    "        \"\"\"\n",
    "        return self.pipeline.transform(examples)\n",
    "\n",
    "    def show_top10(self):\n",
    "        \"\"\"\n",
    "        prints the top 10 features for the positive class and the \n",
    "        top 10 features for the negative class. \n",
    "        \"\"\"\n",
    "        capital_features = CapitalTransformer()\n",
    "        ner_features = NERTransformer()\n",
    "        spl_features = SpecialTransformer()\n",
    "        POS_features = POSTransformer()\n",
    "        spl_features.get_feature_names()\n",
    "        features = self.vectorizer.get_feature_names()+self.vectorizer_ngram.get_feature_names()+self.tropeTransformer.get_feature_names()+capital_features.get_feature_names()+spl_features.get_feature_names()+ner_features.get_feature_names()+POS_features.get_feature_names() \n",
    "        feature_names = np.asarray(features)\n",
    "        top10 = np.argsort(self.logreg.coef_[0])[-50:]\n",
    "        bottom10 = np.argsort(self.logreg.coef_[0])[:50]\n",
    "        print(\"Pos: %s\" % \" \".join(feature_names[top10]))\n",
    "        print(\"Neg: %s\" % \" \".join(feature_names[bottom10]))\n",
    "        \n",
    "    def train_model(self, random_state=1234):\n",
    "        \"\"\"\n",
    "        Method to read in training data from file, and \n",
    "        train Logistic Regression classifier. \n",
    "        \n",
    "        :param random_state: seed for random number generator \n",
    "        \"\"\"\n",
    "        \n",
    "        from sklearn.linear_model import LogisticRegression \n",
    "        \n",
    "        # load data \n",
    "        dfTrain = pd.read_csv(\"../data/spoilers/train.csv\")\n",
    "        \n",
    "        # get training features and labels \n",
    "        self.X_train = self.build_train_features(dfTrain)\n",
    "        self.y_train = np.array(dfTrain[\"spoiler\"], dtype=int)\n",
    "        \n",
    "        # train logistic regression model.  !!You MAY NOT CHANGE THIS!! \n",
    "        self.logreg = LogisticRegression(random_state=random_state)\n",
    "        self.logreg.fit(self.X_train, self.y_train)\n",
    "        self.coeffs = self.logreg.coef_[0]\n",
    "        \n",
    "    def model_predict(self):\n",
    "        \"\"\"\n",
    "        Method to read in test data from file, make predictions\n",
    "        using trained model, and dump results to file \n",
    "        \"\"\"\n",
    "        \n",
    "        # read in test data \n",
    "        dfTest  = pd.read_csv(\"../data/spoilers/test.csv\")\n",
    "        \n",
    "        # featurize test data \n",
    "        self.X_test = self.get_test_features(dfTest)\n",
    "        \n",
    "        # make predictions on test data \n",
    "        pred = self.logreg.predict(self.X_test)\n",
    "        \n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        scores = cross_val_score(self.logreg, self.X_train, self.y_train, cv=6)\n",
    "        print(\"\\nTFIDF Crossvalidation Scores: \",end=\" \")\n",
    "        print(scores)\n",
    "        print(\"TFIDF Mean Accuracy in Cross-Validation = {:.3f}\".format(scores.mean()))\n",
    "        \n",
    "        # dump predictions to file for submission to Kaggle  \n",
    "        pd.DataFrame({\"spoiler\": np.array(pred, dtype=bool)}).to_csv(\"prediction.csv\", index=True, index_label=\"Id\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos:  back scifiwritershavenosenseofscale  his  anyonecandie cliffhanger  as  andimustscream disneydeath promotiontoparent soundtrackdissonance cliffhangercopout anguisheddeclarationoflove reallifewritestheplot batmangambit everybodyissingle furagainstfang secretkeeper  it  alljustadream itspersonal noending hopespot heroicsacrifice attemptedrape neverfoundthebody backfromthedead parentalabandonment corruptcorporateexecutive villainousbreakdown chekhovsgun stabletimeloop deadallalong heroicbsod nicejobbreakingithero nonamegiven themole  her   out  driventosuicide killedoffforreal ohcrap thereveal bittersweetending foreshadowing whamline xanatosgambit whamepisode  he   ,   . \n",
      "Neg:  show deadpansnarker show  domcom catchphrase abc spinoff  i  flanderization gilligancut theotherdarrin thecastshowoff  ofte heyitsthatguy gameshow  usua often stockfootage like  usual thebbc catfight nbc celebrityparadox bitcharacter  one  itsawonderfulplot fantasticcomedy nicehat gettingcrappasttheradar  drew than   tim  showntheirwork friendswithbenefits jerkass ascendedextra hideyourpregnancy screwedbythenetwork goodisnotnice sitcom dysfunctionalfamily workcom metaguy rippedfromtheheadlines ruleoffunny deliberatevaluesdissonance  tend throwitin familyvaluesvillain\n",
      "\n",
      "TFIDF Crossvalidation Scores:  [ 0.71629073  0.70526316  0.69473684  0.71829574  0.7037594   0.69573935]\n",
      "TFIDF Mean Accuracy in Cross-Validation = 0.706\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the FeatEngr clas \n",
    "feat = FeatEngr()\n",
    "\n",
    "# Train your Logistic Regression classifier \n",
    "feat.train_model(random_state=1230)\n",
    "\n",
    "# Shows the top 10 features for each class \n",
    "feat.show_top10()\n",
    "\n",
    "# Make prediction on test data and produce Kaggle submission file \n",
    "feat.model_predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import word_tokenize\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "class SubjectBodyExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        \n",
    "        features = np.recarray(shape=(len(data),),\n",
    "                               dtype=[('sentence', object), ('trope', object)])\n",
    "        features['sentence'] = list(data['sentence'])\n",
    "        features['trope'] = list(data['trope'])\n",
    "\n",
    "        return features\n",
    "\n",
    "class SentimentTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, examples):\n",
    "        sentiment = ['compound']\n",
    "        #sentiment = ['neu']\n",
    "        X = np.zeros((len(examples), len(sentiment)))\n",
    "        s=sia()\n",
    "        for counter, example in enumerate(examples):\n",
    "            res = s.polarity_scores(example)\n",
    "            X[counter,:] = np.array([res.get(k) for k in sentiment])\n",
    "        return csr_matrix(X)\n",
    "    \n",
    "class TropeSentimentTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, examples):\n",
    "        sentiment = ['pos','neg','neu']\n",
    "        X = np.zeros((len(examples), len(sentiment)))\n",
    "        s=sia()\n",
    "        for counter, example in enumerate(examples):\n",
    "            res = s.polarity_scores(\" \".join(re.sub( r\"([A-Z])\", r\" \\1\", example).split()))\n",
    "            X[counter,:] = np.array([res.get(k) for k in sentiment])\n",
    "        return csr_matrix(X)\n",
    "\n",
    "class CapitalTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_names = [\"NumOfCapitalLetters\"]\n",
    "        pass\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "    \n",
    "    def transform(self, examples):\n",
    "        X = np.zeros((len(examples), 1))\n",
    "        for counter, example in enumerate(examples):\n",
    "            X[counter,:] = np.array([sum(1 for c in example if c.isupper())])\n",
    "        return csr_matrix(X)\n",
    "\n",
    "class SpecialTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_names = [\"dead\", \"die\",  \"kill\", \"killed\", \"suicide\", \"shot\", \"staked\", \"stabbed\", \"finale\",\"died\", \"dies\", \"dying\", \"death\", \"survived\", \"survive\", \"stab\", \"revealed\", \"realize\", \"realizes\",  \"averted\", \"averts\", \"finally\", \"final\", \"alive\", \"murder\", \"murdered\", \"murderer\", \"killer\", \"learns\", \"married\", \"marries\", \"wedding\", \"realizes\", \"actually\", \"!\", \"pregnant\", \"end\", \"ending\", \"killing\", \"kills\", \"eventually\", \"reason\", \"discover\", \"discovered\", \"shoot\", \"truth\", \"ultimately\"]  \n",
    "        pass\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "    \n",
    "    def transform(self, examples):\n",
    "        wnl = WordNetLemmatizer()\n",
    "        letters = [\"dead\", \"die\",  \"kill\", \"killed\", \"suicide\", \"shot\", \"staked\", \"stabbed\", \"finale\",\"died\", \"dies\", \"dying\", \"death\", \"survived\", \"survive\", \"stab\", \"revealed\", \"realize\", \"realizes\",  \"averted\", \"averts\", \"finally\", \"final\", \"alive\", \"murder\", \"murdered\", \"murderer\", \"killer\", \"learns\", \"married\", \"marries\", \"wedding\", \"realizes\", \"actually\", \"!\", \"pregnant\", \"end\", \"ending\", \"killing\", \"kills\", \"eventually\", \"reason\", \"discover\", \"discovered\", \"shoot\", \"truth\", \"ultimately\"]  \n",
    "        X = np.zeros ((len(examples), len(letters)))\n",
    "        for counter, example in enumerate(examples):\n",
    "            #doc_prepped = [wnl.lemmatize(t) for t in word_tokenize(example)]\n",
    "            #count = len(letters.intersection(doc_prepped))\n",
    "            #X[counter,:] = np.array(count)\n",
    "            X[counter,:] = np.array([example.count(letter) for letter in letters])\n",
    "        return csr_matrix(X)\n",
    "    \n",
    "class WordCountTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, examples):\n",
    "        X = np.zeros((len(examples), 1))\n",
    "        for counter, example in enumerate(examples):\n",
    "            X[counter,:] = np.array(len(example.split()))\n",
    "        return csr_matrix(X)\n",
    "\n",
    "class NERTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_names = [\"B-PERSON\",\"I-PERSON\",\"B-ORGANIZATION\",\"I-ORGANIZATION\"]\n",
    "        pass\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "    \n",
    "    def transform(self, examples):\n",
    "        from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "        from nltk.chunk import tree2conlltags\n",
    "        \n",
    "        types=['B-PERSON','I-PERSON','B-ORGANIZATION','I-ORGANIZATION']\n",
    "        X = np.zeros((len(examples), len(types)))\n",
    "        for counter, example in enumerate(examples):\n",
    "            ne_tree = ne_chunk(pos_tag(word_tokenize(example)))\n",
    "            iob_tagged = tree2conlltags(ne_tree)\n",
    "            iob_string = \" \".join(x[2] for x in iob_tagged)\n",
    "            X[counter,:] = np.array([iob_string.count(t) for t in types])\n",
    "        return csr_matrix(X)\n",
    "\n",
    "class TropeNERTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, examples):\n",
    "        self.feature_names = [\"B-PERSON\",\"I-PERSON\",\"B-ORGANIZATION\",\"I-ORGANIZATION\"]\n",
    "        \n",
    "        types=['B-PERSON','I-PERSON','B-ORGANIZATION','I-ORGANIZATION']\n",
    "        X = np.zeros((len(examples), len(types)))\n",
    "        for counter, example in enumerate(examples):\n",
    "            ne_tree = ne_chunk(pos_tag(re.sub( r\"([A-Z])\", r\" \\1\", example).split()))\n",
    "            iob_tagged = tree2conlltags(ne_tree)\n",
    "            iob_string = \" \".join(x[2] for x in iob_tagged)\n",
    "            X[counter,:] = np.array([iob_string.count(t) for t in types])\n",
    "        return csr_matrix(X)\n",
    "\n",
    "class POSTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_names = ['VB','NN','NNP','VBZ']\n",
    "        pass\n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, examples):\n",
    "        from nltk.data import load\n",
    "        tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "        \n",
    "        \n",
    "        #tags = [k for k in tagdict.keys()]\n",
    "        tags = ['VB','NN','NNP','VBZ']\n",
    "        X = np.zeros((len(examples), len(tags)))\n",
    "        for counter, example in enumerate(examples):\n",
    "            example_tags = list(word_tag_tuple[1] for word_tag_tuple in nltk.pos_tag(word_tokenize(example)))\n",
    "            X[counter,:] = np.array([example_tags.count(letter) for letter in tags])\n",
    "        return csr_matrix(X)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [25 points] Problem 2: Motivation and Analysis \n",
    "***\n",
    "\n",
    "The job of the written portion of the homework is to convince the grader that:\n",
    "\n",
    "- Your new features work\n",
    "- You understand what the new features are doing\n",
    "- You had a clear methodology for incorporating the new features\n",
    "\n",
    "Make sure that you have examples and quantitative evidence that your features are working well. Be sure to explain how you used the data (e.g., did you have a validation set? did you do cross-validation?) and how you inspected the results. In addition, it is very important that you show some kind of an **error analysis** throughout your process.  That is, you should demonstrate that you've looked at misclassified examples and put thought into how you can craft new features to improve your model. \n",
    "\n",
    "A sure way of getting a low grade is simply listing what you tried and reporting the Kaggle score for each. You are expected to pay more attention to what is going on with the data and take a data-driven approach to feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the features explored, I used a cross validation over the train set. I used a cross validation fold of 5, which splits the training set into 5 equal samples. I kept records of the features and parameters used, along with their accuracy.\n",
    "\n",
    "I started with the baseline model, assessed the overall accuracy and found the most influential features, and also inspected the top50 influential features for both the classes. Based on false positives and false negatives, I tried to design features to reduce those errors.\n",
    "\n",
    "**Features I explored were:**\n",
    "* Bag of words on the sentence using TfidfVectorizer\n",
    "* Bag of characters on the sentence using TfidfVectorizer\n",
    "* Bag of words on the trope using CountVectorizer\n",
    "* Sentiment analysis on the sentence\n",
    "* Named Entity Recognition on the sentences\n",
    "* Part of Speech Tagging\n",
    "* Manual observation of training data\n",
    "* Capital letters in sentence\n",
    "\n",
    "**Reasons for choosing different features**\n",
    "\n",
    "**1. Bag of words on the sentence using TfidfVectorizer:**\n",
    "\n",
    "The baseline system used CountVectorizer as a bag of words model. For any text classification task, the bag of words from the training set will be a good fit as a feature. But the CountVectorizer just counts the word frequencies. Simple as that. With the TFIDFVectorizer the value increases proportionally to count, but is offset by the frequency of the word in the corpus. This helps to adjust for the fact that some words appear more frequently. A collection of unigrams (what bag of words is) cannot capture phrases and multi-word expressions, effectively disregarding any word order dependence. Instead of building a simple collection of unigrams (n=1), one might prefer a collection of bigrams (n=2), where occurrences of pairs of consecutive words are counted. Using Tfidf instead of the CountVectorizer gave a good accuracy. It increased from 0.62 in baseline to 0.64.\n",
    "\n",
    "\n",
    "**2. Bag of characters on the sentence using TfidfVectorizer:**\n",
    "\n",
    "The bag of words model doesn’t account for potential misspellings or word derivations. We can alternatively consider a collection of character n-grams, a representation resilient against misspellings and derivations. char_wb is used to pad the characters on each side with a whitespace. char_wb is especially interesting for languages that use white-spaces for word separation as it generates significantly less noisy features than the raw char variant in that case. For such languages it can increase both the predictive accuracy and convergence speed of classifiers trained using such features while retaining the robustness with regards to misspellings and word derivations. The bad of words on characters was a good choice for the training data at hand since it is highly possible to see noise in data extracted form tropes. The accuracy inturn increased from 0.64 to 0.68.\n",
    "\n",
    "\n",
    "**3. Bag of words on the trope using CountVectorizer:**\n",
    "\n",
    "Using trope as a feature resulted in a very high accuracy prediction. The accuracy jumped from .68 to .70. Analysing the data, we find that explicitly stated content(spoiler) have similar tropes. This is the reason for the trope giving high accuracy. On observing the top few features, it is seen that trope has occured in both the positive and negative top features. It is thus clear that trope is a heavy influencer for the training and validation data.\n",
    "\n",
    "**4. Sentiment analysis on the sentence:**\n",
    "\n",
    "Analysing the sentiment of the sentence helps with analysing the category of it. Sentiment analysis as a text classification task is an example of this. So on a similar note, analysing the sentiment was thought of as a feature and implemented. This however didnt improve the accuracy much on the training data but on the test data the accuracy remained the same. Hence to avoid overfitting, the feature wasn't considered.\n",
    "\n",
    "**5. Named Entity Recognition on the sentences**\n",
    "\n",
    "Named entity recognition is used to get the proper nouns like Persons and organizations from a text. It is related to Part of Speech Tagging. So finding the this should tell more about the underlying contextual meaning of the nouns in the sentence. Implementing this improved the accuracy by 0.2\n",
    "\n",
    "**6. Part of Speech Tagging**\n",
    "\n",
    "Part of Speech tagging helps with identifying the context of the sentence. Rather than looking at the entriety of vocabulary, we can focus on the 45 tags associated with every word in the vocabulary. This fetaure implemented gave a really good accuracy improvement since it will try to reduce the overfitting due to specific words and concentrate on the general case of the vocabulary by only considering the part of speech unique to that sentence.\n",
    "\n",
    "**7.Manual observation of training data**\n",
    "\n",
    "This is hand extraction of features using the training data at hand. We can look at the training data and find the features that are most important for both the classes and occur most frequently in both of them to handpick them. I did this and picked up a set of features to add to my list and this improved my accuracy by 0.1.\n",
    "\n",
    "**8. Capital letters in sentence**\n",
    "\n",
    "The number of capital letters was a good indictaion and also improved accuracy beacuse it is a form of exclamation when narrating a text. When we convert a spoken dialogue from a tv serial into written form we naturally tend to capitalize to stress the importance of certain words in the situation. Hence this led to an improved accuracy on test data.\n",
    "\n",
    "**Reference:**\n",
    "\n",
    "http://scikit-learn.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph below is a histogram which shows the distribution of weights that were found by fitting the data to the logistic regression model. This looks like a normally distributed graph and since ML models assume that your input features are generally normally distributed the graph below is a good example if this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtkAAAGNCAYAAADEum3iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzCRbkiRJ6tj/D+VhUcLSIRSRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f156f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mycolors = {\"blue\": \"steelblue\", \"red\": \"#a76c6e\", \"green\": \"#6a9373\"}\n",
    "values = [i for i in sorted(feat.coeffs) if i<4]\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n",
    "n, bins, patches = ax.hist(values, 100, normed=1, facecolor=mycolors[\"red\"], alpha=0.75, edgecolor=\"white\")\n",
    "ax.set_xlabel(\"Coefficients\", fontsize=16)\n",
    "ax.set_ylabel(\"Frequency\", fontsize=16);\n",
    "ax.set_title(\"Feature Weights Visulization\", fontsize=20);\n",
    "ax.grid(alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints \n",
    "***\n",
    "\n",
    "- Don't use all the data until you're ready. \n",
    "\n",
    "- Examine the features that are being used.\n",
    "\n",
    "- Do error analyses.\n",
    "\n",
    "- If you have questions that aren’t answered in this list, feel free to ask them on Piazza.\n",
    "\n",
    "### FAQs \n",
    "***\n",
    "\n",
    "> Can I heavily modify the FeatEngr class? \n",
    "\n",
    "Totally.  This was just a starting point.  The only thing you cannot modify is the LogisticRegression classifier.  \n",
    "\n",
    "> Can I look at TV Tropes?\n",
    "\n",
    "In order to gain insight about the data yes, however, your feature extraction cannot use any additional data (beyond what I've given you) from the TV Tropes webpage.\n",
    "\n",
    "> Can I use IMDB, Wikipedia, or a dictionary?\n",
    "\n",
    "Yes, but you are not required to. So long as your features are fully automated, they can use any dataset other than TV Tropes. Be careful, however, that your dataset does not somehow include TV Tropes (e.g. using all webpages indexed by Google will likely include TV Tropes).\n",
    "\n",
    "> Can I combine features?\n",
    "\n",
    "Yes, and you probably should. This will likely be quite effective.\n",
    "\n",
    "> Can I use Mechanical Turk?\n",
    "\n",
    "That is not fully automatic, so no. You should be able to run your feature extraction without any human intervention. If you want to collect data from Mechanical Turk to train a classifier that you can then use to generate your features, that is fine. (But that’s way too much work for this assignment.)\n",
    "\n",
    "> Can I use a Neural Network to automatically generate derived features? \n",
    "\n",
    "No. This assignment is about your ability to extract meaningful features from the data using your own experimentation and experience.\n",
    "\n",
    "> What sort of improvement is “good” or “enough”?\n",
    "\n",
    "If you have 10-15% improvement over the baseline (on the Public Leaderboard) with your features, that’s more than sufficient. If you fail to get that improvement but have tried reasonable features, that satisfies the requirements of assignment. However, the extra credit for “winning” the class competition depends on the performance of other students.\n",
    "\n",
    "> Where do I start?  \n",
    "\n",
    "It might be a good idea to look at the in-class notebook associated with the Feature Engineering lecture where we did similar experiments. \n",
    "\n",
    "\n",
    "> Can I use late days on this assignment? \n",
    "\n",
    "You can use late days for the write-up submission, but the Kaggle competition closes at **4:59pm on Friday February 23rd**\n",
    "\n",
    "> Why does it say that the competition ends at 11:59pm when the assignment says 4:59pm? \n",
    "\n",
    "The end time/date are in UTC.  11:59pm UTC is equivalent to 4:59pm MST.  Kaggle In-Class does not allow us to change this. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
